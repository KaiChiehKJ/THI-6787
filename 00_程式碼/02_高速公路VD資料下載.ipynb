{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b47f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "import shutil\n",
    "import tarfile\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import gzip\n",
    "from ProcessBasic import * \n",
    "\n",
    "# logfile = os.path.join(os.getcwd(), 'VD_logfile.txt')\n",
    "logfile = None  # 預設為 None，在 main() 裡設定\n",
    "\n",
    "def download_VD(url, downloadpath):\n",
    "    \"\"\"\n",
    "    下載指定網址的 XML 檔案到指定位置。\n",
    "\n",
    "    Args:\n",
    "        url (str): 要下載的 XML 檔案網址。\n",
    "        downloadpath (str): 檔案下載後的儲存路徑（包含檔案名稱）。\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # 檢查 HTTP 狀態碼，如有錯誤則拋出異常\n",
    "\n",
    "        with open(downloadpath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        updatelog(file=logfile, text = f\"ERROR:下載時發生錯誤, {e}\")\n",
    "    except Exception as e:\n",
    "        updatelog(file=logfile, text = f\"ERROR: 發生錯誤, {e}\")\n",
    "\n",
    "def read_xml(xml_file_path, return_raw=False):\n",
    "    \"\"\"\n",
    "    讀取並解析 XML 檔案。\n",
    "\n",
    "    Args:\n",
    "        xml_file_path (str): XML 檔案路徑。\n",
    "        return_raw (bool): 是否返回原始 XML 內容，預設為 False (返回解析後的 XML 根節點)。\n",
    "\n",
    "    Returns:\n",
    "        ElementTree.Element 或 str: 解析後的 XML 根節點，或原始 XML 內容 (若 return_raw=True)。\n",
    "        None: 如果檔案未找到或解析失敗。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(xml_file_path, 'r', encoding='utf-8') as f:\n",
    "            xml_content = f.read()\n",
    "        \n",
    "        if return_raw:\n",
    "            return xml_content  # 返回原始 XML 內容\n",
    "        \n",
    "        tree = ET.ElementTree(ET.fromstring(xml_content))\n",
    "        return tree.getroot()  # 返回解析後的 XML 根節點\n",
    "    except FileNotFoundError:\n",
    "        updatelog(file=logfile, text = f\"ERROR:未找到{xml_file_path}\")\n",
    "        return None\n",
    "    except ET.ParseError as e:\n",
    "        updatelog(file=logfile, text = f\"ERROR: 解析 XML 檔案時發生錯誤, {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_vd_xml(xml_content):\n",
    "    \"\"\"\n",
    "    解析 VD XML 資料並轉換為 DataFrame。\n",
    "\n",
    "    Args:\n",
    "        xml_content (str): XML 內容。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 解析後的 DataFrame。\n",
    "    \"\"\"\n",
    "    namespace = {'ns': 'http://traffic.transportdata.tw/standard/traffic/schema/'}\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # 解析全域資訊\n",
    "    update_time = root.find('ns:UpdateTime', namespace).text\n",
    "    update_interval = root.find('ns:UpdateInterval', namespace).text\n",
    "    authority_code = root.find('ns:AuthorityCode', namespace).text\n",
    "\n",
    "    # 解析 VD 資料\n",
    "    data = []\n",
    "    for vd in root.findall('ns:VDs/ns:VD', namespace):\n",
    "        vdid = vd.find('ns:VDID', namespace).text\n",
    "        sub_authority_code = vd.find('ns:SubAuthorityCode', namespace).text\n",
    "        bi_directional = vd.find('ns:BiDirectional', namespace).text\n",
    "        vd_type = vd.find('ns:VDType', namespace).text\n",
    "        location_type = vd.find('ns:LocationType', namespace).text\n",
    "        detection_type = vd.find('ns:DetectionType', namespace).text\n",
    "        position_lon = vd.find('ns:PositionLon', namespace).text\n",
    "        position_lat = vd.find('ns:PositionLat', namespace).text\n",
    "        road_id = vd.find('ns:RoadID', namespace).text\n",
    "        road_name = vd.find('ns:RoadName', namespace)\n",
    "        road_name = road_name.text if road_name is not None else ''  # 防止 AttributeError\n",
    "        road_class = vd.find('ns:RoadClass', namespace)\n",
    "        road_class = road_class.text if road_class is not None else ''\n",
    "        location_mile = vd.find('ns:LocationMile', namespace)\n",
    "        location_mile = location_mile.text if location_mile is not None else ''\n",
    "\n",
    "        # 解析 RoadSection\n",
    "        start = vd.find('ns:RoadSection/ns:Start', namespace)\n",
    "        end = vd.find('ns:RoadSection/ns:End', namespace)\n",
    "        start_text = start.text if start is not None else ''\n",
    "        end_text = end.text if end is not None else ''\n",
    "\n",
    "        # 解析 DetectionLinks\n",
    "        detection_links = vd.findall('ns:DetectionLinks/ns:DetectionLink', namespace)\n",
    "        for link in detection_links:\n",
    "            link_id = link.find('ns:LinkID', namespace).text\n",
    "            bearing = link.find('ns:Bearing', namespace).text\n",
    "            road_direction = link.find('ns:RoadDirection', namespace).text\n",
    "            lane_num = link.find('ns:LaneNum', namespace).text\n",
    "            actual_lane_num = link.find('ns:ActualLaneNum', namespace).text\n",
    "\n",
    "            data.append([\n",
    "                update_time, update_interval, authority_code, vdid, sub_authority_code, bi_directional,\n",
    "                link_id, bearing, road_direction, lane_num, actual_lane_num, vd_type, location_type,\n",
    "                detection_type, position_lon, position_lat, road_id, road_name, road_class, start_text, end_text, location_mile\n",
    "            ])\n",
    "\n",
    "    # 轉成 DataFrame\n",
    "    columns = [\n",
    "        \"UpdateTime\", \"UpdateInterval\", \"AuthorityCode\", \"VDID\", \"SubAuthorityCode\", \"BiDirectional\",\n",
    "        \"LinkID\", \"Bearing\", \"RoadDirection\", \"LaneNum\", \"ActualLaneNum\", \"VDType\", \"LocationType\",\n",
    "        \"DetectionType\", \"PositionLon\", \"PositionLat\", \"RoadID\", \"RoadName\", \"RoadClass\", \"Start\", \"End\", \"LocationMile\"\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "def get_text(element, tag, namespace):\n",
    "    found = element.find(tag, namespace)\n",
    "    return found.text if found is not None else None\n",
    "\n",
    "def parse_vdlive_xml(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 命名空間\n",
    "    namespace = {'ns': 'http://traffic.transportdata.tw/standard/traffic/schema/'}\n",
    "\n",
    "    # 解析全局欄位\n",
    "    update_time = get_text(root, 'ns:UpdateTime', namespace)\n",
    "    update_interval = get_text(root, 'ns:UpdateInterval', namespace)\n",
    "    authority_code = get_text(root, 'ns:AuthorityCode', namespace)\n",
    "\n",
    "    # 存放資料的列表\n",
    "    data = []\n",
    "\n",
    "    # 遍歷 VDLive\n",
    "    for vd in root.findall(\".//ns:VDLive\", namespace):\n",
    "        vdid = get_text(vd, \"ns:VDID\", namespace)\n",
    "        # status = get_text(vd, \"ns:Status\", namespace)\n",
    "        status = get_text(vd, \"ns:status\", namespace)\n",
    "        data_collect_time = get_text(vd, \"ns:DataCollectTime\", namespace)\n",
    "\n",
    "        for link_flow in vd.findall(\".//ns:LinkFlow\", namespace):\n",
    "            link_id = get_text(link_flow, \"ns:LinkID\", namespace)\n",
    "\n",
    "            for lane in link_flow.findall(\".//ns:Lane\", namespace):\n",
    "                lane_id = get_text(lane, \"ns:LaneID\", namespace)\n",
    "                lane_type = get_text(lane, \"ns:LaneType\", namespace)\n",
    "                speed = get_text(lane, \"ns:Speed\", namespace)\n",
    "                occupancy = get_text(lane, \"ns:Occupancy\", namespace)\n",
    "\n",
    "                for vehicle in lane.findall(\".//ns:Vehicle\", namespace):\n",
    "                    vehicle_type = get_text(vehicle, \"ns:VehicleType\", namespace)\n",
    "                    volume = get_text(vehicle, \"ns:Volume\", namespace)\n",
    "                    speed_2 = get_text(vehicle, \"ns:Speed\", namespace)\n",
    "\n",
    "                    # 加入記錄\n",
    "                    data.append([\n",
    "                        update_time, update_interval, authority_code, vdid, link_id, \n",
    "                        lane_id, lane_type, speed, occupancy, vehicle_type, volume, \n",
    "                        speed_2, status, data_collect_time\n",
    "                    ])\n",
    "\n",
    "    # 建立 DataFrame\n",
    "    columns = [\n",
    "        \"UpdateTime\", \"UpdateInterval\", \"AuthorityCode\", \"VDID\", \"LinkID\", \n",
    "        \"LaneID\", \"LaneType\", \"Speed\", \"Occupancy\", \"VehicleType\", \"Volume\", \n",
    "        \"SpeedAvg\", \"Status\", \"DataCollectTime\"\n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def vdlive_preliminary_process(df, vdlist = None):\n",
    "    df['Volume'] = df['Volume'].astype('int64')\n",
    "    df['Status'] = df['Status'].astype('int64')\n",
    "    df = df[(df['Volume'] > 0) & (df['Status'] == 0)]\n",
    "\n",
    "    if vdlist:\n",
    "        df = df[df['VDID'].isin(vdlist)]\n",
    "\n",
    "    return df.reset_index(drop = True)\n",
    "\n",
    "def VDfolder(datatype = 'VDlive'):\n",
    "    savelocation = create_folder(os.path.join(os.getcwd(),'..', '01_資料初步彙整', '04_高速公路VD資料', datatype))\n",
    "    rawdatafolder = create_folder(os.path.join(savelocation, '0_rawdata'))\n",
    "    mergefolder = create_folder(os.path.join(savelocation, '1_merge'))\n",
    "    excelfolder = create_folder(os.path.join(savelocation, '2_excel'))\n",
    "    return rawdatafolder, mergefolder, excelfolder\n",
    "\n",
    "def get_vd(date = None):\n",
    "    vdfolder = create_folder(os.path.join(os.getcwd(), 'VD'))\n",
    "    vdxmlfolder = create_folder(os.path.join(vdfolder, 'xml'))\n",
    "    vdpath = os.path.join(os.path.join(vdxmlfolder, 'VD.xml'))\n",
    "\n",
    "    if date:\n",
    "        create_folder(os.path.join(vdxmlfolder,date))\n",
    "        vdpath = os.path.join(vdxmlfolder,date,'VD_0000.xml.gz')\n",
    "        url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VD_0000.xml.gz'\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(vdpath, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            extract_gz(vdpath, create_folder(os.path.join(vdpath, '..', 'temp')))\n",
    "            vdpath = os.path.abspath(os.path.join(vdpath, '..', 'temp', 'VD_0000.xml'))\n",
    "        else:\n",
    "            error_message = f\"ERROR: {vdpath} 檔案無法下載，狀態碼: {response.status_code}，回應內容: {response.text}\"\n",
    "    else:\n",
    "        download_VD(url = 'https://tisvcloud.freeway.gov.tw/history/motc20/VD.xml', downloadpath = vdpath)\n",
    "    VD = read_xml(vdpath, return_raw=True)\n",
    "    VD = parse_vd_xml(VD)\n",
    "\n",
    "    if date:\n",
    "        outputname = os.path.join(vdfolder, f'VD_{date}.csv')\n",
    "    else:\n",
    "        outputname = os.path.join(vdfolder, 'VD.csv')\n",
    "    VD.to_csv(outputname, index = False)\n",
    "    return VD\n",
    "\n",
    "def extract_gz(destfile, downloadfolder):\n",
    "    try:\n",
    "        # 確保目標資料夾存在\n",
    "        os.makedirs(downloadfolder, exist_ok=True)\n",
    "        \n",
    "        # 取得解壓後的檔名\n",
    "        extracted_file = os.path.join(downloadfolder, os.path.basename(destfile).replace('.gz', ''))\n",
    "        \n",
    "        # 解壓檔案\n",
    "        with gzip.open(destfile, 'rb') as f_in:\n",
    "            with open(extracted_file, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        return extracted_file\n",
    "\n",
    "    except Exception as e:\n",
    "        updatelog(file=logfile, text = f\"ERROR: 解壓失敗：{e}\")\n",
    "        return None\n",
    "    \n",
    "def download_and_extract_VD(url, datatype, date, downloadfolder, keep = False):\n",
    "    '''針對高公局交通資料庫的格式進行下載'''\n",
    "    hourlist = [f\"{i:02d}\" for i in range(24)]\n",
    "    minutelist = [f\"{i:02d}\" for i in range(0, 60, 1)]\n",
    "    downloadfolder = create_folder(os.path.join(downloadfolder, date))\n",
    "    gzdownloadfolder = create_folder(os.path.join(downloadfolder, '壓縮檔'))\n",
    "    for hour in hourlist:\n",
    "        for minute in minutelist:\n",
    "            # https://tisvcloud.freeway.gov.tw/history/motc20/VD/20241205/VDLive_2315.xml.gz\n",
    "            downloadurl = f\"{url}/{date}/VDLive_{hour}{minute}.xml.gz\"\n",
    "            destfile = os.path.join(gzdownloadfolder, f\"VDLive_{hour}{minute}.xml.gz\")\n",
    "            checkfile = os.path.abspath(os.path.join(destfile,'..','..',f\"VDLive_{hour}{minute}.xml\"))\n",
    "            check_exist_bool = check_pathexist(checkfile)\n",
    "            if check_exist_bool:\n",
    "                updatelog(file=logfile, text = f\"WARN: {checkfile} 已經存在，不進行下載\")\n",
    "            else:\n",
    "                response = requests.get(downloadurl)\n",
    "                if response.status_code == 200:\n",
    "                    with open(destfile, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                        updatelog(file=logfile, text = f\"INFO: {destfile} 下載成功\")\n",
    "                    extract_gz(destfile, downloadfolder)\n",
    "                    updatelog(file=logfile, text = f\"INFO: {destfile} 解壓縮成功\")\n",
    "                else:\n",
    "                    error_message = f\"ERROR: {destfile} 檔案無法下載，狀態碼: {response.status_code}，回應內容: {response.text}\"\n",
    "                    updatelog(file=logfile, text=error_message)\n",
    "                    # updatelog(file=logfile, text = f\"ERROR: {destfile} 檔案無法下載\")\n",
    "    os.remove(gzdownloadfolder)\n",
    "    return downloadfolder\n",
    "\n",
    "def cleanVD(df):\n",
    "    df[\"Direction\"] = df[\"VDID\"].str.extract(r\"VD-[A-Z0-9]+-([A-Z])-\")\n",
    "    df = df.reindex(columns=['VDID', 'Status','DataCollectTime', 'Direction', 'LaneID', 'Speed', 'Occupancy', 'VehicleType', 'Volume'])\n",
    "    df.columns = ['vdid', 'status', 'datacollecttime', 'vsrdir', 'vsrid', 'speed', 'laneoccupy', 'carid', 'volume']\n",
    "    return df \n",
    "\n",
    "def VD_volume(df, roadselectlist = None):\n",
    "    df['UpdateTime'] = pd.to_datetime(df['UpdateTime'] )\n",
    "    df['Date'] = df['UpdateTime'].dt.strftime('%Y/%m/%d')\n",
    "    df['Hour'] = df['UpdateTime'].dt.strftime('%H')\n",
    "    df[\"Direction\"] = df[\"VDID\"].str.extract(r\"VD-[A-Z0-9]+-([A-Z])-\")\n",
    "    df = pd.pivot_table(\n",
    "        df,\n",
    "        index=['VDID', 'Date', 'Hour', 'Direction'],\n",
    "        columns='VehicleType',\n",
    "        values='Volume',\n",
    "        aggfunc='sum'  # 可依需求改成 'mean', 'max' 等\n",
    "    ).reset_index()\n",
    "    df[['L','S','T']] = df[['L','S','T']].fillna(0)\n",
    "    df = df.groupby(['VDID', 'Date', 'Hour', 'Direction']).agg({'S':'sum', 'T':'sum', 'L':'sum'}).reset_index()\n",
    "    df = df.rename(columns = {'VDID':'設備代碼',\n",
    "                              'Date':'日期',\n",
    "                              'Hour':'小時',\n",
    "                              'Direction':'車道方向', \n",
    "                              'S':'小型車',\n",
    "                              'T':'聯結車',\n",
    "                              'L':'大型車'})\n",
    "    \n",
    "    if roadselectlist : \n",
    "        vd_info_dict = {'N1': '國道1號',\n",
    "                    'N10': '國道10號',\n",
    "                    'N1H': '國道1號高架',\n",
    "                    'N1K': '國道1號',\n",
    "                    'N2': '國道2號',\n",
    "                    'N3': '國道3號',\n",
    "                    'N3A': '國道3號甲',\n",
    "                    'N3K': '國道3號',\n",
    "                    'N3N': '國道3號',\n",
    "                    'N4': '國道4號',\n",
    "                    'N5': '國道5號',\n",
    "                    'N6': '國道6號',\n",
    "                    'N8': '國道8號',\n",
    "                    'T66': '台66',\n",
    "                    'T68': '台68',\n",
    "                    'T72': '台72',\n",
    "                    'T74': '台74',\n",
    "                    'T76': '台76',\n",
    "                    'T78': '台78',\n",
    "                    'T82': '台82',\n",
    "                    'T84': '台84',\n",
    "                    'T86': '台86',\n",
    "                    'T88': '台88'}\n",
    "        \n",
    "        df['國道'] = df['設備代碼'].apply(lambda x: x.split('-')[1])\n",
    "        df['國道'] = df['國道'].map(vd_info_dict)\n",
    "        df = df[df['國道'].isin(roadselectlist)]\n",
    "        df = df.drop(columns = ['國道'])\n",
    "    \n",
    "    df['小型車分時PCU'] = df['小型車'] * 1.0\n",
    "    df['聯結車分時PCU'] = df['聯結車'] * 1.4\n",
    "    df['大型車分時PCU'] = df['大型車'] * 1.4\n",
    "\n",
    "\n",
    "    return df \n",
    "\n",
    "def calculate_peak_hour(VD_Data):\n",
    "    # 計算合計分時PCU\n",
    "    updatelog(file=logfile, text = f\"INFO: 計算合計分時PCU\")\n",
    "    VD_Data['合計分時PCU'] = VD_Data['小型車分時PCU'] + VD_Data['大型車分時PCU'] + VD_Data['聯結車分時PCU']\n",
    "\n",
    "    # 找到每組(設備代碼, 日期)的尖峰時段\n",
    "    updatelog(file=logfile, text = f\"INFO: 找到每組(設備代碼, 日期)的尖峰時段\")\n",
    "    VD_Data['尖峰時段'] = VD_Data.groupby(['設備代碼', '日期'])['合計分時PCU'].transform(max)\n",
    "\n",
    "    # 標示尖峰小時\n",
    "    VD_Data['尖峰小時'] = np.where(VD_Data['尖峰時段'] == VD_Data['合計分時PCU'], '*', 'NA')\n",
    "\n",
    "    # 提取尖峰時段資料\n",
    "    peak_hour = VD_Data[VD_Data['尖峰小時'] == '*'].reset_index(drop=True)\n",
    "    peak_hour['尖峰時段'] = peak_hour['小時']\n",
    "    peak_hour.rename(columns={'合計分時PCU': '尖峰小時PCU'}, inplace=True)\n",
    "\n",
    "    updatelog(file=logfile, text = f\"INFO:  合併 尖峰時段、尖峰小時PCU 兩個欄位\")\n",
    "    # 合併尖峰時段資料\n",
    "    VD_Data = VD_Data.drop('尖峰時段', axis=1)\n",
    "    VD_Data = VD_Data.merge(peak_hour[['尖峰時段', '尖峰小時PCU', '設備代碼', '日期']], on=['設備代碼', '日期'], how='left')\n",
    "\n",
    "    # 彙總資料並計算尖峰率\n",
    "    agg_columns = ['設備代碼', '日期', '車道方向', '尖峰時段', '尖峰小時PCU']\n",
    "    VD_Data_Day = VD_Data.groupby(agg_columns)[['小型車', '聯結車', '大型車', '小型車分時PCU', '聯結車分時PCU', '大型車分時PCU', '合計分時PCU']].sum().reset_index()\n",
    "    \n",
    "    # 整理欄位順序與名稱\n",
    "    VD_Data_Day = VD_Data_Day[['設備代碼', '日期', '小型車', '聯結車', '大型車', '小型車分時PCU', '聯結車分時PCU', '大型車分時PCU', '合計分時PCU', '尖峰時段', '尖峰小時PCU']]\n",
    "    VD_Data_Day.columns = ['設備代碼', '日期', '小型車', '聯結車', '大型車', '小型車全日PCU', '聯結車全日PCU', '大型車全日PCU', '合計全日PCU', '尖峰時段', '尖峰小時PCU']\n",
    "\n",
    "    # 計算尖峰率\n",
    "    VD_Data_Day['尖峰率'] = round(VD_Data_Day['尖峰小時PCU'] / VD_Data_Day['合計全日PCU'], 3)\n",
    "\n",
    "    return VD_Data_Day\n",
    "\n",
    "def VDlive (datelist , datatype = 'VD_live', vdlist = None, roadselectlist = None):\n",
    "    '''\n",
    "    VDlive 函數包含下載、解壓縮、過濾、合併等步驟\n",
    "    \n",
    "    Args:\n",
    "        datelist (list): 要下載的日期清單，以%Y%M%D的形式list組成。\n",
    "        datatype (str): 檔案下載後的儲存類型\n",
    "        vdlist (list):需要過濾的清單\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # datatype = 'VD_live'\n",
    "    url = \"https://tisvcloud.freeway.gov.tw/history/motc20/VD/\" \n",
    "    rawdatafolder, mergefolder, excelfolder = VDfolder(datatype=datatype)\n",
    "    for date in datelist :\n",
    "        year = date[:4]\n",
    "        month = date[4:6]\n",
    "\n",
    "        updatelog(file=logfile, text = f\"INFO: 開始下載{date}的{datatype}檔案\")\n",
    "        # Step1 : 下載\n",
    "        try:\n",
    "            dowloadfilefolder = os.path.join(rawdatafolder, date)\n",
    "            dowloadfilefolder = download_and_extract_VD(url, datatype, date, downloadfolder = rawdatafolder, keep = False)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Step2 : xml -> csv\n",
    "        updatelog(file=logfile, text = f\"INFO: 開始下載讀取{date}的{datatype}原始xml檔案\")\n",
    "        dowloadfilefolder = os.path.join(rawdatafolder, date)\n",
    "        delete_folders([os.path.join(dowloadfilefolder,'壓縮檔')])\n",
    "        updatelog(file=logfile, text = f\"INFO: 刪除{date}的{datatype}原始gz壓縮檔案\")\n",
    "\n",
    "        filelist = findfiles(filefolderpath=dowloadfilefolder, filetype='.xml')\n",
    "        VDlivemergename = os.path.join(mergefolder, f\"{date}.csv\")\n",
    "        check_path_exist_bool = check_pathexist(VDlivemergename)\n",
    "        if check_path_exist_bool == False: # 如果已經有merge過的檔案不重複處理 (怕使用者下載不同時間)\n",
    "            updatelog(file=logfile, text = f\"INFO: 開始讀取{date}的{datatype}xml資料\")\n",
    "            VDLive = []\n",
    "            for filepath in filelist:\n",
    "                # filepath = filelist[0]\n",
    "                updatelog(file=logfile, text = f\"INFO: 正在讀取{filepath}的xml資料\")\n",
    "                try:\n",
    "                    df = parse_vdlive_xml(filepath)\n",
    "                    df = vdlive_preliminary_process(df, vdlist=vdlist)\n",
    "                    VDLive.append(df)\n",
    "                except:\n",
    "                    updatelog(file=logfile, text = f\"ERROR: {filepath}原始xml資料出現失誤\")\n",
    "            VDLive = pd.concat(VDLive, ignore_index=True)\n",
    "            updatelog(file=logfile, text = f\"INFO: {date}dataframe 合併成功\")\n",
    "            VDLive.to_csv(VDlivemergename, index = False)\n",
    "            updatelog(file=logfile, text = f\"INFO: {date}資料存於 {VDlivemergename}\")\n",
    "        else :\n",
    "            updatelog(file=logfile, text = f\"WARN: 資料夾中已將有{date}的合併資料，不進行更新\")\n",
    "            VDLive = pd.read_csv(VDlivemergename)\n",
    "\n",
    "        VDLiveclean = cleanVD(VDLive)\n",
    "        updatelog(file=logfile, text = f\"INFO: {date}dataframe 轉為五分鐘格式\")\n",
    "        VDlivecleanfolder = create_folder(os.path.join(mergefolder, '符合原本五分鐘格式'))\n",
    "        VDlivecleanname =  os.path.join(VDlivecleanfolder,f'{date}.csv')\n",
    "        VDLiveclean.to_csv(VDlivecleanname, index = False)\n",
    "        updatelog(file=logfile, text = f\"INFO: {date}(轉為五分鐘格式) 存於 {VDlivecleanname}\")\n",
    "\n",
    "\n",
    "        # Step3 : 統計每個小時通過Volume\n",
    "        VDLive = VD_volume(VDLive, roadselectlist)\n",
    "        updatelog(file=logfile, text = f\"INFO: {date}資料進行正規化\")\n",
    "        VDvolumecountfolder = create_folder(os.path.join(excelfolder, '正規化分時PCU',year,month))\n",
    "        VDexcelname = os.path.join(VDvolumecountfolder, f'{date}.xlsx')\n",
    "        VDLive.to_excel(VDexcelname, index=False)\n",
    "        updatelog(file=logfile, text = f\"INFO: {date}正規化資料輸出於 {VDexcelname}\")\n",
    "        reformat_excel(VDexcelname)\n",
    "    \n",
    "    # Step 4 : 把整個月分進行統計\n",
    "    lastyear = datetime.now().year - 1\n",
    "    updatelog(file=logfile, text = f\"INFO: 開始整併 {lastyear}年 VD通過量資料\")\n",
    "    VDvolumecountfolder = create_folder(os.path.join(excelfolder, '正規化分時PCU', str(lastyear)))\n",
    "    \n",
    "    volume_lastyear = read_combined_dataframe(findfiles(filefolderpath=VDvolumecountfolder, filetype='.xlsx'))\n",
    "    volumeoutputname = os.path.join(create_folder(os.path.join(excelfolder, '正規化分時PCU', '整合')), f'{lastyear}VD 正規化彙整資料20240413.xlsx')\n",
    "    volume_lastyear.to_excel(volumeoutputname, index=False)\n",
    "    updatelog(file=logfile, text = f\"INFO: {lastyear}年VD通過量資料輸出：{volumeoutputname}\")\n",
    "    reformat_excel(volumeoutputname)\n",
    "\n",
    "\n",
    "    # Step 5 : 計算尖峰小時PCU\n",
    "    updatelog(file=logfile, text = f\"INFO: 開使轉換尖峰小時PCU {lastyear}年\")\n",
    "    volume_lastyear = calculate_peak_hour(volume_lastyear)\n",
    "    volumeoutputname = os.path.join(create_folder(os.path.join(excelfolder, '正規化及尖峰小時')), f'{lastyear}VD 正規化及尖峰小時20240413.xlsx')\n",
    "    volume_lastyear.to_excel(volumeoutputname, index = False)\n",
    "    updatelog(file=logfile, text = f\"INFO: 尖峰小時PCU資料輸出為: {volumeoutputname}\")\n",
    "    reformat_excel(volumeoutputname)\n",
    "\n",
    "def main():\n",
    "    # 0. 定義我們的logfile\n",
    "    global logfile\n",
    "    logfile = os.path.join(os.getcwd(), 'VD_logfile.txt')\n",
    "    refreshlog(file = logfile, day = 1)\n",
    "    \n",
    "    # # 1. 下載VD靜態資料\n",
    "    # vdtable = get_vd()\n",
    "    # updatetime = str(vdtable['UpdateTime'][0])[:10]\n",
    "    # updatelog(file=logfile, text = f\"INFO: 下載最新VD靜態資料, 版本更新時間為:{updatetime}. \")\n",
    "    # reformat_excel(excel_path=os.path.join(os.getcwd(), 'VD', 'VD.csv'), allsheet=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 2. 下載VD Live 資料\n",
    "    # 2-1 調整下載的資料區間\n",
    "    starttime = \"2024-04-13\"\n",
    "    endtime = \"2024-04-14\"\n",
    "    datelist = getdatelist(endtime,starttime) # 下載的時間區間清單\n",
    "    datelist = [\"20250619\", \"20250621\"]\n",
    "\n",
    "    # 2-2 需要過濾出來的VD清單\n",
    "    SelectVD = [\n",
    "        \"VD-N3-S-129.653-N-LOOP\", \"VD-N3-N-129.623-N-LOOP\",\n",
    "        \"VD-N3-S-161.105-M-LOOP\", \"VD-N3-N-159.490-M-RS\",\n",
    "        \"VD-N1-S-156.040-M-RS\", \"VD-N1-N-156.010-M-RS\",\n",
    "        \"VD-N3-S-166.681-M-RS\", \"VD-N3-N-166.688-M-RS\",\n",
    "        \"VD-N1-S-163.450-M-RS\", \"VD-N1-N-163.450-M-RS\",\n",
    "        \"VD-N3-S-189.722-M-RS\", \"VD-N3-N-189.737-M-RS\",\n",
    "        \"VD-N1-S-191.001-M-LOOP\", \"VD-N1-N-190.490-M-LOOP\"\n",
    "    ]\n",
    "\n",
    "    # 2-3 需要過濾出來的路線\n",
    "    # SelectRoad = ['國道1號']\n",
    "\n",
    "    VDlive(datelist = datelist , datatype = 'VD_live', vdlist = SelectVD)\n",
    "\n",
    "# 執行 main()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
